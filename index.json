[{"content":"Edge devices deployed out in the field pose very different operational, environmental, and business challenges from those of cloud computing. These motivate different engineering trade-offs for Kubernetes at the far edge than for cloud or near-edge scenarios. MicroShift\u0026rsquo;s design goals cater to this:\n make frugal use of system resources (CPU, memory, network, storage, etc.), tolerate severe networking constraints, update (resp. roll back) securely, safely, speedily, and seamlessly (without disrupting workloads), and build on and integrate cleanly with edge-optimized OSes like Fedora IoT and RHEL for Edge, while providing a consistent development and management experience with standard OpenShift.  We believe these properties should also make MicroShift a great tool for other use cases such as Kubernetes applications development on resource-constrained systems, scale testing, and provisioning of lightweight Kubernetes control planes.\nWatch this end-to-end MicroShift provisioning demo video to get a first impression of MicroShift deployed onto a RHEL for edge computing device and managed through Open Cluster Management.\nWarning\nMicroShift is still early days and moving fast. Features are missing. Things break. But you can still help shape it, too.\n ","permalink":"https://microshift.io/docs/about/about/","summary":"MicroShift is optimized for edge computing.","title":"About"},{"content":"Build Dependencies Install the required binaries:\n git make golang glibc podman # if building containerized, as recommended  # Fedora/CentOS sudo dnf install \\  git \\  make \\  golang \\  glibc-static # Ubuntu sudo apt install \\  #git \\ build-essential \\  # provides Make golang-go \\  glibc To install podman and build containerized MicroShift (recommended), find the appropriate guide for your respective system: Install Podman\nBuilding MicroShift In Linux Container (recommended) Build a container image with the MicroShift binary using Podman or Docker:\nmake microshift podman images # note the microshift image name, or `podman tag, push` to tag or push to a registry The built image can then be run by replacing the latest released image name (quay.io/microshift/microshift:4.7.0-0.microshift-2021-08-31-224727-linux-amd64) in the MicroShift-Containerized documentation with the locally built image name.\nBuilding MicroShift on the Host MicroShift can be built directly on the host after installing the build-time dependencies. When using RHEL ensure the system is registered and run the following before installing the prerequisites.\nARCH=$( /bin/arch ) sudo subscription-manager repos --enable \u0026#34;codeready-builder-for-rhel-8-${ARCH}-rpms\u0026#34; Next clone the repository and cd into it, then build with make:\ngit clone https://github.com/redhat-et/microshift.git cd microshift make build Warning\nwhen building or running ARM64 container images, Linux host environments must have the qemu-user-static package installed. E.g. on Fedora: dnf install qemu-user-static.\n Running MicroShift Locally It is recommended to run from a container, but it is possible to run the MicroShift binary locally with the following commands. First, run the install script to prepare the host environment.\ncd microshift CONFIG_ENV_ONLY=true ./install.sh sudo ./microshift run (optional) Developing with Vagrant It is possible to use Vagrant for VM provisioning, however it is not necessary.\nFind a guide on how to install it for your system here.\nOnce Vagrant is installed, create a Vagrant box for the operating system of choice. For this example we will be looking at a fedora 34 cloud image, however you can substitute any vagrant image of your choice.\nFirst, navigate to the MicroShift directory on your host system, or another designated directory where we will be storing the Vagrantfile.\nNext, download the vagrant image. For this example we will use a fedora 34 cloud image:\nvagrant box add fedora/34-cloud-base Depending on the image, Vagrant will ask you to select a Virtualization provider, just select the first one.\nOnce that downloads, initialize the repository for launching your image:\nvagrant init fedora/34-cloud-base Running this command will create a Vagrantfile in your working directory which is used to configure your vagrant box.\nBefore starting the Vagrant box, increase the amount of RAM available to the system. To do this, edit the Vagrantfile and configure your provider settings to include the following:\nconfig.vm.provider \u0026#34;libvirt\u0026#34; do |v| # provides 3GB of memory v.memory = 3072 # for parallelization v.cpus = 2 end The value of config.vm.provider depends on the provider you selected when you ran vagrant add earlier. For example, if you selected virtualbox then the first line should be: config.vm.provider \u0026quot;virtualbox\u0026quot; do |v|\nNow start the VM:\nvagrant up Once the VM is up, connect to it:\nvagrant ssh Once ssh\u0026rsquo;d into the vagrant instance, return to the Build Dependencies section to begin local development.\n(Extra Optional) Connecting VSCode to Vagrant If using VSCode, you can connect to your vagrant box with a few extra steps.\nIncreasing Memory Requirements Since VSCode leans more on the heavy side of development, the RAM usage on your Vagrant environment can go up to 5GB, and therefore we will need to modify the Vagrantfile to increase the amount of available RAM from 3GB to 5GB (or 6GB if you want to be safe). To do this, set v.memory to the following in your Vagrantfile:\n# provides 5GB of memory v.memory = 5120 # provides 6GB of memory v.memory = 6144 Setting up an SSH Profile First we need to ask Vagrant for an SSH config file. From your host machine, run:\nvagrant ssh-config \u0026gt; ssh-config.conf You can edit the ssh-config.conf file to change the hostname from default to vagrant to be more easily identifiable, but that\u0026rsquo;s up to you. :)\nHere\u0026rsquo;s an example of a working SSH config file:\nHost default HostName 127.0.0.1 User vagrant Port 2222 UserKnownHostsFile /dev/null StrictHostKeyChecking no PasswordAuthentication no IdentityFile /path/to/microshift/.vagrant/machines/default/virtualbox/private_key IdentitiesOnly yes LogLevel FATAL Next, you\u0026rsquo;ll want to install the Remote - SSH extension from the VSCode Marketplace\nWith the extension installed, you\u0026rsquo;ll click on the green bottom in the bottom-left corner of VSCode to open a dropdown menu for SSH options:\nSelect the option to open an SSH configuration file: Next you\u0026rsquo;ll want to navigate to the \u0026ldquo;Remote Explorer\u0026rdquo; tab on the left-hand side of VSCode, then select on the vagrant target (default if you haven\u0026rsquo;t renamed it) and click on the button to connect to it in a remote window.\n(Credits to Andr√©s Lopez for this guide: Connect Visual Studio Code with Vagrant in your local machine )\n","permalink":"https://microshift.io/docs/developer-documentation/local-development/","summary":"Building and running MicroShift for local development","title":"Local Development"},{"content":"The is a documentation of MicroShift\u0026rsquo;s design goals, design principles, and fundamental design decisions. For details on specific feature enhancements, please refer to the corresponding low-level design documents.\nDesign Goals MicroShift aims at meeting all of the following design goals:\n  Optimized for field-deployment:\n Provisioning and replacing devices running MicroShift is \u0026ldquo;plug\u0026amp;play\u0026rdquo;1; MicroShift does not add friction to this.  e.g. auto-configuring, auto-clustering   MicroShift works seamlessly under adverse network conditions.  e.g. disconnected or rarely connected, NAT\u0026rsquo;ed or firewalled, changing IP addresses, IPv4 or v6, high latency / low bandwidth, no control over local network (DNS, DHCP, LBN, GW), connectivity via LTE dongle (i.e. no LAN)   MicroShift operates autonomously; it does not require external orchestration. MicroShift is safe to change1; it has means to automatically recover from faulty software or configuration updates that would render it unmanageable or non-operational. MicroShift is secure1 even in environments without physical access security.    Production-grade:\n MicroShift supports deployments with 1 or 3 control plane and 0..N worker instances. MicroShift can be deployed containerized on Podman or Docker or non-containerized via RPM and managed via systemd; it is compatible with rpm-ostree-based systems. MicroShift\u0026rsquo;s lifecycle is decoupled from the underlying OS\u0026rsquo;s lifecycle. MicroShift can be deployed such that updates or changes to it do not disrupt running workloads. MicroShift meets DISA STIG and FedRAMP security requirements; it runs as non-privileged workload and supports common CVE and auditing workflows. MicroShift allows segregation between the \u0026ldquo;edge device administrator\u0026rdquo; and the \u0026ldquo;edge service development and operations\u0026rdquo; personas.  the former being responsible for device+OS lifecycle and installing MicroShift as a workload, the latter being responsible for services on and resources of the MicroShift cluster   MicroShift provides application-level events and metrics for observability.    Usability:\n MicroShift does not require Kubernetes-expertise to configure, deploy, or lifecycle-manage it; Linux admins will find it behaves like any other Linux workload. MicroShift does not require Kubernetes-experts to learn new techniques; Kubernetes developers and operations teams will find it supports their tools and processes. MicroShift is highly opinionated, working with minimal/no configuration out of the box, but offers escape hatches for advanced users.    Small resource footprint:\n MicroShift makes frugal use of system resources. It runs on \u0026lt;1GB RAM and \u0026lt;1 CPU core (Intel Atom- or ARM Cortex-class). It consumes \u0026lt;500MB on the wire (per install/update) and \u0026lt;1GB at rest (excl. etcd state).    Consistency with OpenShift:\n MicroShift runs all workloads that OpenShift runs, except those which depend on OpenShift\u0026rsquo;s cluster operators. MicroShift clusters can be managed like OpenShift clusters through Open Cluster Management, except where functions depend on OpenShift\u0026rsquo;s cluster operators.    Design Principles When deciding between different design options, we follow the following principles:\n Minimal core: We keep MicroShift to a minimal set of functionality, but provide mechanisms for extension.  Discriminator: If a functionality can be added post-cluster-up with reasonable effort, then it should not be part of the MicroShift core/binary.   Minimal configuration: We minimize the number of configuration parameters exposed to users. Where parameters cannot be avoided, we provide robust defaults or try to auto-configure them.  Discriminator: If a parameter can be infered from another parameter, auto-detected, or only covers rare use cases, then likely it should not be exposed to users.   Robustness to failure modes: We expect and gracefully handle failure modes stemming from field-deployment and that MicroShift is just an app on somebody else\u0026rsquo;s OS it cannot control. Production over Development: We engineer for production-deployments, not Kubernetes application development environments. Frugal use of resources: We are mindful of MicroShift\u0026rsquo;s resources consumption, considering all resources (memory, CPU, storage I/O, network I/O). No premature resouce optimization: We do not attempt to squeeze out the last 20% of resouces, e.g. by patching or compressing code, inventing lighter-weight components, etc. Ease-of-Use: We make MicroShift intutive and simple to use, even if that means providing fewer features and options. Prefer well-established mechanisms: We meet users where they are by letting them use well-established tools and patterns rather than requiring them to learn new ones. Cheap Control Plane Restarts: We keep MicroShift restarts cheap (= fast, low resource consumption, low workload impact, etc.) as it is our default model for software and configuration changes. Alignment with OpenShift: We reuse OpenShift\u0026rsquo;s code, operational logic, and production chain tools and processes where possible with reasonable effort, unless this would conflict with MicroShift\u0026rsquo;s goals.  Design Decisions Overall Architecture  MicroShift is an application deployed onto a running OS, preferably as container on podman, managed through systemd. As such, it cannot assume any responsiblity or control over the device or OS it runs on, including OS software or configuration updates or typical device management tasks such as configuring host CA certs or host telemetry. MicroShift runs as a single binary embedding as goroutines only those services strictly necessary to bring up a minimal Kubernetes/OpenShift control and data plane. Motivation:  Maximizes reproducibility; cluster will come up fully or not at all. Does not require external orchestration, for example through operators, and allows for very fast start-up/update times. Makes it simple to grok as workload for a a Linux admin persona, works well / easier to implement with systemd. Smaller resource footprint has not been a motivation, it may be a welcome side-effect.   MicroShift provides a small, optional set of infrastructure services to support common use cases and reuses OpenShift\u0026rsquo;s container images for these:  openshift-dns, openshift-router, service-ca, local storage provider   MicroShift instances (processes) run directly on the host or containerized on Podman. They can take on the roles of Control Plane, Node, or both:  Instances with Control Plane role run etcd and the Kubernetes and OpenShift control plane services. As these services don\u0026rsquo;t require a kubelet, pure Control Plane instances are not nodes in the Kubernetes sense and require fewer system privileges. Instances with Node role run a kubelet (and thus register as node) and kube-proxy and interface with CRI-O for running workloads. They may thus require higher system privileges.   While it\u0026rsquo;s possible to run a single MicroShift instance with both Control Plane and Node roles, there may be reasons to run two instances - one Control Plane and one Node - on the same host, e.g. to run the Control Plane with fewer privileges for security reasons. Implementation decisions should consider this. MicroShift does not bundle any OS user space! Bundling makes maintenance and security hard, breaks compliance. Instead, user space is provided by the host OS, the container image base layer or a sidecar container.  CLI  The microshift binary runs the Control Plane / Node process, it is not a tool to manage or be clients to those processes (like oc or kubeadmin). This is reflected in the sub-commands and paraemters offered by it, e.g. using the run verb (which implies run-to-cancel/run-to-completion) instead of start/stop verb-pairs (which imply asynch commands that return immediately). For consistency and to play nicely with systemd, we avoid command line parameters that would need to be different between invokations (e.g. first-run vs subsequent runs) or instantiations (e.g. 1st Control Plane instances vs. 2nd or 3rd Control Plane instance).  Configuration  MicroShift uses a strictly declarative style of configuration. MicroShift uses as few configuration options as possible. Where it provides configuration options, they are intuitive and have sensible defauls respectively are auto-configured. MicroShift is preferably configured through config files, but allows overriding of parameters via environment variables (for use in containers, systemd) and command line flags (for ad-hoc use). MicroShift can use both user-local and system-wide configuration.  Security  MicroShift instances should run with least privileges. In particular Control Plane-only instances should run completely non-privileged. MicroShift should minimize the number of open network ports. In particular Node-only instances should not open any listening ports. Assume there is no way to access the device or MicroShift instance from remote, i.e. no SSH access nor Kubernetes API access (for kubectl). Instead, communication is always initiated from the device / cluster towards the management system. Open issues / questions:  Model for joining nodes to MicroShift clusters. Model for certificate rotation. Requirements for FedRAMP and DISA STIG compliance. How to leverage / support the host OS\u0026rsquo;s features for FIDO Device Onboard, remote attestation, integrity measurement architecture. How to support secure supply chain.    Networking  Host networking is configured by device management. MicroShift has to work with what it\u0026rsquo;s been given by the host OS. No Multus. Open issues / questions:  Lightweight CNI? API Load Balancing? Service Load Balancing? Ingress?    Storage  MicroShift defaults to local ephemeral storage (enough for basic use cases). Open issues / questions:  Provide escape hatch to add own CSI (which?).    Production / Supply Chain / Release Management  MicroShift vendors OKD source code without modification. Where it deploys container images for additional services, it deploys OKD\u0026rsquo;s published container images, not the OpenShift downstream\u0026rsquo;s. MicroShift\u0026rsquo;s versioning scheme follows OKD\u0026rsquo;s. This scheme signals the base OpenShift version (4.x) and order/age of builds, but intentionally avoids signaling patch level, backward compatibility (as SemVer, for example), or stability. We ensure the tip of our development branch is deployable and while MicroShift is still early days and experimental we expect developers (and users who want the \u0026ldquo;latest\u0026rdquo;) to build \u0026amp; deploy from source. Releases are mainly provided for convenience to users that just want to give MicroShift a quick try without friction. They are cut irregularly, e.g. to make a new feature available. When rebasing onto a new OKD version, we vendor that version\u0026rsquo;s packages and update the container image digests of the infrastructure services MicroShift deploys, i.e. the \u0026ldquo;release metadata\u0026rdquo; is baked into the MicroShift binary. Eventually, we expect there to be a \u0026ldquo;MicroShift Release Image\u0026rdquo; that is based on / derived from the OpenShift Release Image: It references the MicroShift container image plus the subset of container images shared with and published by OpenShift. Defining a release image should allow to reuse the proven OpenShift CI and release tooling later.    when used in combination with an edge-optimized OS like RHEL 4 Edge or Fedora IoT\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://microshift.io/docs/getting-started/design/","summary":"MicroShift\u0026rsquo;s design goals, design principles, and fundamental design decisions.","title":"Design"},{"content":"Minimum specs In order to run MicroShift, you will need at least:\n 2 CPU cores 2GB of RAM ~124MB of free storage space for the MicroShift binary 64-bit CPU (although 32-bit is technically possible, if you\u0026rsquo;re up for the challenge)  For barebone development the minimum requirement is 3GB of RAM, though this can increase if you are using resource-intensive development tools.\nOS Requirements The all-in-one containerized MicroShift can run on Windows, MacOS, and Linux.\nCurrently, the MicroShift binary is known to be supported on the following Operating Systems:\n Fedora 33/34 CentOS 8 Stream RHEL 8 CentOS 7 Ubuntu 20.04  It may be possible to run MicroShift on other systems, however they haven\u0026rsquo;t been tested so you may run into issues.\n","permalink":"https://microshift.io/docs/getting-started/system-requirements/","summary":"MicroShift system requirements","title":"System Requirements"},{"content":"WIP Content coming soon ","permalink":"https://microshift.io/docs/user-documentation/accessing-microshift/","summary":"How to access a running cluster","title":"Accessing a MicroShift Cluster"},{"content":"WIP Content coming soon  issue triage, meaning of tags, reviews, PRs, squash merges enhancement proposal process  ","permalink":"https://microshift.io/docs/developer-documentation/contribute/","summary":"Contribution processes for MicroShift.","title":"Contributing to MicroShift"},{"content":"WIP Content coming soon  config.yaml vs env vars vs command line logging  ","permalink":"https://microshift.io/docs/user-documentation/configuring/","summary":"Configuration options with MicroShift","title":"Configuring MicroShift"},{"content":"WIP - More details coming soon.\n","permalink":"https://microshift.io/docs/getting-started/microshift-edge-computing/","summary":"MicroShift can be run from container with host CRI-O or non-containerized from an RPM.","title":"Edge Computing With MicroShift"},{"content":"WIP Content coming soon ","permalink":"https://microshift.io/docs/developer-documentation/release-management/","summary":"MicroShift rebase and release process.","title":"Release Management"},{"content":"OSX WIP: More Content Coming Soon\nWindows WIP: More Content Coming Soon\nMicroShift All-In-One Image MicroShift All-In-One includes everything required to run MicroShift in a single container image. This deployment mode is recommended for development and testing only.\nRun MicroShift All-In-One as a Systemd Service Copy microshift-aio.service unit file to /etc/systemd and the microshift-aio run script to /usr/bin\ncurl -o /etc/systemd/system/microshift-aio.service https://raw.githubusercontent.com/redhat-et/microshift/main/packaging/systemd/microshift-aio.service curl -o /usr/bin/microshift-aio https://raw.githubusercontent.com/redhat-et/microshift/main/packaging/systemd/microshift-aio Now enable and start the service. The KUBECONFIG location will be written to /etc/microshift-aio/microshift-aio.conf. If the microshift-data podman volume does not exist, the systemd service will create one.\nsystemctl enable microshift-aio --now source /etc/microshift-aio/microshift-aio.conf Verify that MicroShift is running.\nkubectl get pods -A Stop microshift-aio service\nsystemctl stop microshift-aio Note\nStopping microshift-aio service does not remove the Podman volume microshift-data. A restart will use the same volume.\n Run MicroShift All-In-One Image Without Systemd First, enable the following SELinux rule:\nsetsebool -P container_manage_cgroup true Next, create a container volume:\nsudo podman volume create microshift-data The following example binds localhost the container volume to /var/lib\nsudo podman run -d --rm --name microshift-aio --privileged -v /lib/modules:/lib/modules -v microshift-data:/var/lib -p 6443:6443 microshift-aio You can access the cluster either on the host or inside the container\nAccess the Cluster Inside the Container Execute the following command to get into the container:\nsudo podman exec -ti microshift-aio bash Inside the container, install kubectl:\nexport ARCH=$(uname -m |sed -e \u0026#34;s/x86_64/amd64/\u0026#34; |sed -e \u0026#34;s/aarch64/arm64/\u0026#34;) curl -LO \u0026#34;https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/${ARCH}/kubectl\u0026#34; \u0026amp;\u0026amp; \\ chmod +x ./kubectl \u0026amp;\u0026amp; \\ mv ./kubectl /usr/local/bin/kubectl Inside the container, run the following to see the pods:\nexport KUBECONFIG=/var/lib/microshift/resources/kubeadmin/kubeconfig kubectl get pods -A Access the Cluster From the Host Linux export KUBECONFIG=$(podman volume inspect microshift-data --format \u0026#34;{{.Mountpoint}}\u0026#34;)/microshift/resources/kubeadmin/kubeconfig kubectl get pods -A -w MacOS docker cp microshift-aio:/var/lib/microshift/resources/kubeadmin/kubeconfig ./kubeconfig kubectl get pods -A -w --kubeconfig ./kubeconfig Windows docker.exe cp microshift-aio:/var/lib/microshift/resources/kubeadmin/kubeconfig .\\kubeconfig kubectl.exe get pods -A -w --kubeconfig .\\kubeconfig Build All-In-One Container Image Build With Locally Built Binary make microshift-aio FROM_SOURCE=\u0026#34;true\u0026#34; Build With Latest Released Binary Download make microshfit-aio QuickStart Local Deployment for Testing Only To give MicroShift a try, simply install a recent test version (we don\u0026rsquo;t provide stable releases yet) on a Fedora-derived Linux distribution (we\u0026rsquo;ve only tested Fedora, RHEL, and CentOS Stream so far) using:\ncurl -sfL https://raw.githubusercontent.com/redhat-et/microshift/main/install.sh | bash This will install MicroShift\u0026rsquo;s dependencies (CRI-O) on the host, install a MicroShift systemd service and start it.\nFor convenience, the script will also add a new \u0026ldquo;microshift\u0026rdquo; context to your $HOME/.kube/config, so you\u0026rsquo;ll be able to access your cluster using, e.g.:\nkubectl get all -A --context microshift or\nkubectl config use-context microshift kubectl get all -A Warning\nWhen installing MicroShift on a system with an older version already installed, it is safest to remove the old data directory and start fresh:\n rm -rf /var/lib/microshift \u0026amp;\u0026amp; rm -r $HOME/.microshift Limitation These instructions are tested on Linux, Mac, and Windows. On MacOS, running containerized MicroShift as non-root is not supported on MacOS.\n","permalink":"https://microshift.io/docs/getting-started/experimental-development/","summary":"MicroShift has been deployed on various platforms and with an All-In-One image","title":"Experimental Development"},{"content":"A ready to go MicroShift environment can be installed from an RPM, along with the systemd unit file to run the MicroShift binary as a service.\nWIP More content coming soon When to use RPM Deployment Architecture MicroShift on host, CRI-O on host\nRPM-OSTree and RHEL for Edge Roll back, Roll Forward Update MicroShift via OS Update ","permalink":"https://microshift.io/docs/user-documentation/deploying-microshift/non-containerized/","summary":"Deploy MicroShift from RPM and run as a systemd service.","title":"MicroShift RPM Install"},{"content":"WIP Content coming soon  bootstrapping and fixed workloads kustomizer to add bootstrap services  ","permalink":"https://microshift.io/docs/user-documentation/adding-services/","summary":"Bootstrapping services to MicroShift","title":"Adding Services"},{"content":"MicroShift can be run from a Linux container with the host CRI-O service and managed with a systemd service.\nWIP More content coming soon When to Use Deployment Architecture MicroShift on podman, CRI-O on host\nSystemd service Auto-Updates Pre-requisites  CRI-O service must be running on the host Before running MicroShift as a systemd service, ensure to update the host crio-bridge.conf as  $ cat /etc/cni/net.d/crio-bridge.conf { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.4.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;crio\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;isGateway\u0026#34;: true, \u0026#34;ipMasq\u0026#34;: true, \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ], \u0026#34;ranges\u0026#34;: [ [{ \u0026#34;subnet\u0026#34;: \u0026#34;10.42.0.0/24\u0026#34; }] ] } } Run MicroShift container as a systemd service Copy microshift-containerized.service unit file to /etc/systemd/system and the microshift-containerized run script to /usr/bin\ncurl -o /etc/systemd/system/microshift.service https://raw.githubusercontent.com/redhat-et/microshift/main/packaging/systemd/microshift-containerized.service curl -o /usr/bin/microshift-containerized https://raw.githubusercontent.com/redhat-et/microshift/main/packaging/systemd/microshift-containerized Now enable and start the service. The KUBECONFIG location will be written to /var/lib/microshift/resources/kubeadmin/kubeconfig.\nsudo systemctl enable microshift --now Verify that MicroShift is running.\nexport KUBECONFIG=/var/lib/microshift/resources/kubeadmin/kubeconfig kubectl get pods -A Stop MicroShift service\nsystemctl stop microshift You can check MicroShift via\nsudo podman ps sudo critcl ps To access the cluster on the host or inside the container\nAccess the cluster inside the container Execute the following command to get into the container:\nsudo podman exec -ti microshift bash Inside the container, run the following to see the pods:\nexport KUBECONFIG=/var/lib/microshift/resources/kubeadmin/kubeconfig kubectl get pods -A Access the cluster on the host Linux export KUBECONFIG=/var/lib/microshift/resources/kubeadmin/kubeconfig kubectl get pods -A -w Auto-Update on demand via Podman Since Podman 3.4, Podman enables users to automate container updates using what are called auto-updates. On a high level, you can configure Podman to check the availability of new images for auto-updates, pull down these new images if needed, and restart the containers.\nConfiguring Podman auto-updates To ensure Podman is checking the fully qualified image path for new images and download them, the systemd file adds a label --label \u0026quot;io.containers.autoupdate=registry\u0026quot; to the microshift container\nExecStart=/usr/bin/podman run \\ --cidfile=%t/%n.ctr-id \\ --cgroups=no-conmon \\ --rm -d --replace \\ --sdnotify=container \\ --label io.containers.autoupdate=registry \\ --privileged --name microshift \\ -v /var/run:/var/run -v /sys:/sys:ro -v /var/lib:/var/lib:rw,rshared -v /lib/modules:/lib/modules -v /etc:/etc\\ -v /run/containers:/run/containers -v /var/log:/var/log \\ -e KUBECONFIG=/var/lib/microshift/resources/kubeadmin/kubeconfig \\ quay.io/microshift/microshift:latest Testing auto-updates Podman auto-update command will look for containers with the label and a systemd service file as described above. If the command finds one container, it will check for a new image, download it, restart the container service.\nsudo podman auto-update --dry-run UNIT CONTAINER IMAGE POLICY UPDATED microshift 2f7fa3962ee0 (microshift) quay.io/microshift/microshift:4.7.0-0.microshift-2021-08-31-224727-linux-amd64 registry false The --dry-run feature allows you to assemble information about which services, containers, and images need updates before applying them. To apply them do\nsudo podman auto-update ","permalink":"https://microshift.io/docs/user-documentation/deploying-microshift/containerized/","summary":"Deploy MicroShift from a Linux container and run as a systemd service.","title":"MicroShift Containerized"},{"content":"WIP Content coming soon Pre-load MicroShift image tarball into CRI-O\n","permalink":"https://microshift.io/docs/user-documentation/disconnected/","summary":"MicroShift can run without internet connectivity.","title":"Disconnected Deployment"},{"content":"On EC2 with RHEL 8.4 service-ca can\u0026rsquo;t be created If you want to run microshift on EC2 RHEL 8.4(cat /etc/os-release), you might find ingress and service-ca will not stay online.\nInside the failing pods, you might find errors as: 10.43.0.1:443: read: connection timed out.\nThis a known issue on RHEL 8.4 and will be resolved in 8.5.\nIn order to work on RHEL 8.4, you may disable the NetworkManager and reboot to resolve this issue.\nExample:\nsystemctl disable nm-cloud-setup.service nm-cloud-setup.timer reboot You can find the details of this EC2 NetworkManager issue tracked at issue.\nOpenShift pods CrashLoopBackOff A few minutes after microshift started, OpenShift pods fall into CrashLoopBackOff.\nIf you check up the journalctl |grep iptables, you may see the following:\nSep 21 19:12:54 ip-172-31-85-30.ec2.internal microshift[1297]: I0921 19:12:54.399365 1297 server_others.go:185] Using iptables Proxier. Sep 21 19:13:50 ip-172-31-85-30.ec2.internal kernel: iptables[2438]: segfault at 88 ip 00007feaf5dc0e47 sp 00007fff6f2fea08 error 4 in libnftnl.so.11.3.0[7feaf5dbc000+16000] Sep 21 19:13:50 ip-172-31-85-30.ec2.internal systemd-coredump[2442]: Process 2438 (iptables) of user 0 dumped core. Sep 21 20:35:57 ip-172-31-85-30.ec2.internal microshift[1297]: E0921 20:35:57.914558 1297 remote_runtime.go:143] StopPodSandbox \u0026#34;1ae45abde0b46d8ea5176b6a00f0e5b4291e6bb496762ca25a4196a5f18d0475\u0026#34; from runtime service failed: rpc error: code = Unknown desc = failed to destroy network for pod sandbox k8s_service-ca-64547678c6-2nxnp_openshift-service-ca_6236deba-fc5f-4915-817d-f8699a4accfc_0(1ae45abde0b46d8ea5176b6a00f0e5b4291e6bb496762ca25a4196a5f18d0475): error removing pod openshift-service-ca_service-ca-64547678c6-2nxnp from CNI network \u0026#34;crio\u0026#34;: running [/usr/sbin/iptables -t nat -D POSTROUTING -s 10.42.0.3 -j CNI-d5d0edec163ce01e4591c1c4 -m comment --comment name: \u0026#34;crio\u0026#34; id: \u0026#34;1ae45abde0b46d8ea5176b6a00f0e5b4291e6bb496762ca25a4196a5f18d0475\u0026#34; --wait]: exit status 2: iptables v1.8.4 (nf_tables): Chain \u0026#39;CNI-d5d0edec163ce01e4591c1c4\u0026#39; does not exist Also, the openshift-ingress pod will fail on:\nI0921 17:36:17.811391 1 router.go:262] router \u0026#34;msg\u0026#34;=\u0026#34;router is including routes in all namespaces\u0026#34; E0921 17:36:17.914638 1 haproxy.go:418] can\u0026#39;t scrape HAProxy: dial unix /var/lib/haproxy/run/haproxy.sock: connect: no such file or directory I0921 17:36:17.948417 1 router.go:579] template \u0026#34;msg\u0026#34;=\u0026#34;router reloaded\u0026#34; \u0026#34;output\u0026#34;=\u0026#34; - Checking http://localhost:80 ...\\n - Health check ok : 0 retry attempt(s).\\n\u0026#34; As a workaround, you can follow steps below:\n  delete flannel daemonset\nkubectl delete ds -n kube-system kube-flannel-ds   restart all the OpenShift pods.\n  This workaround won\u0026rsquo;t affect the single node microshift functionality since the flannel daemonset is used for multi-node MicroShift.\nThis issue is tracked at: #296\n","permalink":"https://microshift.io/docs/user-documentation/troubleshooting/","summary":"MicroShift known issues","title":"Troubleshooting"},{"content":"WIP More content coming soon ","permalink":"https://microshift.io/docs/user-documentation/how-tos/add-image-registry/","summary":"MicroShift image registry","title":"Configure a Local Image Registry"},{"content":"WIP More content coming soon ","permalink":"https://microshift.io/docs/user-documentation/how-tos/gpu-with-microshift/","summary":"GPU support","title":"Containerized MicroShift with GPU Support"},{"content":"WIP More content coming soon ","permalink":"https://microshift.io/docs/user-documentation/how-tos/acm-with-microshift/","summary":"MicroShift with ACM","title":"MicroShift with Advanced Cluster Management"},{"content":"All of the standard Kubernetes management tools can be used to maintain and modify your MicroShift applications. Below we will show some examples using kubectl, kustomize, and helm to deploy and maintain applications.\nExample Applications Metal LB Metal LB is a load balancer that can be used to route traffic to a number of backends.\nCreating the Metal LB namespace and deployment.\nkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/namespace.yaml kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.11.0/manifests/metallb.yaml Once the components are available, a ConfigMap is required to define the address pool for the load balancer to use.\nCreate the Metal LB ConfigMap:\nkubectl create -f - \u0026lt;\u0026lt;EOFapiVersion:v1kind:ConfigMapmetadata:namespace:metallb-systemname:configdata:config:|address-pools: - name: default protocol: layer2 addresses: - 192.168.1.240-192.168.1.250EOFNow we are able to deploy a test application to verify thing are working as expected.\nkubectl create ns test kubectl create deployment nginx -n test --image nginx Create a service:\nkubectl create -f - \u0026lt;\u0026lt;EOFapiVersion:v1kind:Servicemetadata:name:nginxnamespace:testannotations:metallb.universe.tf/address-pool:defaultspec:ports:- port:80targetPort:80selector:app:nginxtype:LoadBalancerEOFVerify the service exists and that an IP address has been assigned.\nkubectl get svc -n test NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx LoadBalancer 10.43.183.104 192.168.1.241 80:32434/TCP 29m Using your browser you can now access the NGINX application by the EXTERNAL-IP provided by the service.\n","permalink":"https://microshift.io/docs/user-documentation/how-tos/example-usage/","summary":"MicroShift operates similar to many other Kubernetes providers. This means that you can use the same tools to deploy and manage your applications.","title":"MicroShift Basic Usage"},{"content":"MicroShift may not have the pull secret for the registry that you are trying to use. For example, MicroShift does not have the pull secret for registry.redhat.io. In order to use this registry, there are a few approaches.\nPulling Container Images From Private Registries Use Podman to Authenticate to a Registry podman login registry.redhat.io Once the podman login is complete, MicroShift will be able to pull images from this registry. This approach works across namespaces.\nThis approach assumes podman is installed. This might not be true for all MicroShift environments. For example, if MicroShift is installed through RPM, CRI-O will be installed as a dependency, but not podman. In this case, one can choose to install podman separately, or use other approaches described below.\nAuthenticate to a Registry With a Pull-Secret The second approach is to create a pull secret, then let the service account use this pull secret. This approach works within a name space. For example, if the pull secret is stored in a json formatted file \u0026ldquo;secret.json\u0026rdquo;,\n# First create the secret in a name space kubectl create secret generic my_pull_secret \\  --from-file=secret.json \\  --type=kubernetes.io/dockerconfigjson # Then attach the secret to a service account in the name space kubectl secrets link default my_pull_secret --for=pull Instead of attaching the secret to a service account, one can also specify the pull secret under the pod spec, Refer to this Kubernetes document for more details.\n","permalink":"https://microshift.io/docs/user-documentation/how-tos/private-registries/","summary":"MicroShift may need access to a private registry. Access can be granted from registry login or from a pull-secret.","title":"Private Registries and Pull Secrets"},{"content":"Join us on Slack! (Invite to the Slack space)\nCommunity meetings are held weekly, Wednesdays at 10:30AM - 11:00AM EST. Be sure to join the community calendar! Click \u0026ldquo;Google Calendar\u0026rdquo; in the lower right hand corner to subscribe.\n","permalink":"https://microshift.io/docs/community/community/","summary":"MicroShift community is growing, we hope you can get involved!","title":"Community"}]